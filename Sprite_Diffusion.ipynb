{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oelin/sprite-diffusion/blob/main/Sprite_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNlv9ryjHXU"
      },
      "source": [
        "# Sprite Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4sR1xxyiftb",
        "outputId": "d2cfb150-9039-4817-8751-425f6097e67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@markdown Install dependencies.\n",
        "\n",
        "!pip -q install einops datasets timm labml labml_helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y_jNNRvZTOL1"
      },
      "outputs": [],
      "source": [
        "#@markdown Implement the noise schedule and sampler.\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def explicit_broadcast(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Explicitly broadcast x to y.\"\"\"\n",
        "\n",
        "    axes = (1,)*max(len(y.shape) - len(x.shape), 0)\n",
        "\n",
        "    return x.view(x.shape + axes)\n",
        "\n",
        "\n",
        "class DDPMSchedule:\n",
        "\n",
        "    def __init__(self, start: float, end: float, steps: int) -> None:\n",
        "\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.steps = steps\n",
        "        self.beta = torch.linspace(start, end, steps)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        noise: torch.Tensor,\n",
        "        step: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Sample from the forward process.\"\"\"\n",
        "\n",
        "        alpha_bar = self.alpha_bar.to(x.device)[step]\n",
        "        a = explicit_broadcast(torch.sqrt(alpha_bar), x)\n",
        "        b = explicit_broadcast(torch.sqrt(1 - alpha_bar), x)\n",
        "\n",
        "        return a*x + b*noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_ddpm(\n",
        "    model: nn.Module,\n",
        "    schedule: DDPMSchedule,\n",
        "    x: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Sample from the model.\"\"\"\n",
        "\n",
        "    for step in reversed(range(schedule.steps)):\n",
        "\n",
        "        z = torch.randn_like(x).to(x.device) if (step > 0) else 0.\n",
        "\n",
        "        alpha = schedule.alpha[step].item()\n",
        "        alpha_bar = schedule.alpha_bar[step].item()\n",
        "        beta = 1 - alpha\n",
        "        noise = model(x, torch.full((x.size(0),), step).to(x.device))\n",
        "\n",
        "        x = 1/math.sqrt(alpha) * (x - beta/math.sqrt(1-alpha_bar)*noise) + math.sqrt(beta)*z\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UjgQ_aeV8ybf"
      },
      "outputs": [],
      "source": [
        "#@markdown Implement the trainer.\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import gc\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "def save_samples(x):\n",
        "\n",
        "    for i, xi in enumerate(x):\n",
        "        xi = (xi - xi.min()) / (xi.max() - xi.min())  # Normalize\n",
        "        transforms.ToPILImage()(xi).resize((256, 256), 0).save(f'./sample-{i:03d}.png')\n",
        "\n",
        "@dataclass\n",
        "class Trainer:\n",
        "\n",
        "    effective_batch_size: int\n",
        "    sample_steps: int\n",
        "    checkpoint_steps: int\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        schedule: DDPMSchedule,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        x_column: str,\n",
        "        steps: int,\n",
        "        device: str,\n",
        "    ) -> None:\n",
        "        \"\"\"Train a model.\"\"\"\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        # sampler = DDPMSampler()\n",
        "        # mixed_precision = torch.float16 if device == 'cuda' else torch.bfloat16\n",
        "\n",
        "        batch_size = dataloader.batch_size\n",
        "        batches_per_step = max(1, self.effective_batch_size // batch_size)\n",
        "\n",
        "        # Training loop.\n",
        "\n",
        "        global_step = 0\n",
        "        bar = tqdm(total=steps)\n",
        "        bar.set_description(f'loss: (pending)')\n",
        "\n",
        "        while global_step < steps:\n",
        "            for batch, examples in enumerate(dataloader):\n",
        "\n",
        "                if len(examples[x_column]) != batch_size:\n",
        "                    continue\n",
        "\n",
        "                x = examples[x_column].to(device)\n",
        "                noise = torch.randn_like(x, device=device)\n",
        "                step = torch.randint(low=0, high=schedule.steps, size=(batch_size,), device=device)\n",
        "                # x_step = get_noised_example(x, noise, step, device)\n",
        "                x_step = schedule.forward(x, noise, step)\n",
        "\n",
        "                # with torch.autocast(device, dtype=mixed_precision):\n",
        "                loss = F.mse_loss(model(x_step, step), noise) / batches_per_step\n",
        "                scaler.scale(loss).backward()\n",
        "                # loss.backward()\n",
        "\n",
        "                # Events.\n",
        "\n",
        "                if (batch + 1) % batches_per_step == 0:\n",
        "\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    # optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    global_step += 1\n",
        "                    bar.set_description(f'loss: {loss.detach().item() * batches_per_step:0.6f}')\n",
        "                    bar.update(1)\n",
        "\n",
        "                if self.sample_steps and (global_step + 1) % self.sample_steps == 0:\n",
        "\n",
        "                    x_shape = list(x.shape)\n",
        "                    x_shape[0] = 4\n",
        "                    x_T = torch.randn(x_shape).to(device)\n",
        "                    x_0 = sample_ddpm(model, schedule, x_T)\n",
        "\n",
        "                    save_samples(x_0)\n",
        "\n",
        "                    del x_T\n",
        "                    del x_0\n",
        "\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                if self.checkpoint_steps and (global_step + 1) % self.checkpoint_steps == 0:\n",
        "\n",
        "                    torch.save(model.state_dict(), './checkpoint.pt')\n",
        "\n",
        "        # Save model.\n",
        "\n",
        "        torch.save(model.state_dict(), './model.pt')\n",
        "\n",
        "        # Clean up.\n",
        "\n",
        "        bar.close()\n",
        "        model.eval()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f5TTHOrSKxZy"
      },
      "outputs": [],
      "source": [
        "#@markdown Implement the model (imported from labml).\n",
        "\n",
        "import math\n",
        "from typing import Optional, Tuple, Union, List\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from labml_helpers.module import Module\n",
        "\n",
        "\n",
        "class Swish(Module):\n",
        "    \"\"\"\n",
        "    ### Swish activation function\n",
        "\n",
        "    $$x \\cdot \\sigma(x)$$\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Embeddings for $t$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int):\n",
        "        \"\"\"\n",
        "        * `n_channels` is the number of dimensions in the embedding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_channels = n_channels\n",
        "        # First linear layer\n",
        "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
        "        # Activation\n",
        "        self.act = Swish()\n",
        "        # Second linear layer\n",
        "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
        "\n",
        "    def forward(self, t: torch.Tensor):\n",
        "        # Create sinusoidal position embeddings\n",
        "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
        "        #\n",
        "        # \\begin{align}\n",
        "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
        "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
        "        # \\end{align}\n",
        "        #\n",
        "        # where $d$ is `half_dim`\n",
        "        half_dim = self.n_channels // 8\n",
        "        emb = math.log(10_000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
        "\n",
        "        # Transform with the MLP\n",
        "        emb = self.act(self.lin1(emb))\n",
        "        emb = self.lin2(emb)\n",
        "\n",
        "        #\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ResidualBlock(Module):\n",
        "    \"\"\"\n",
        "    ### Residual block\n",
        "\n",
        "    A residual block has two convolution layers with group normalization.\n",
        "    Each resolution is processed with two residual blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int,\n",
        "                 n_groups: int = 32, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        * `in_channels` is the number of input channels\n",
        "        * `out_channels` is the number of input channels\n",
        "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
        "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
        "        * `dropout` is the dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Group normalization and the first convolution layer\n",
        "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
        "        self.act1 = Swish()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        # Group normalization and the second convolution layer\n",
        "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
        "        self.act2 = Swish()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        # If the number of input channels is not equal to the number of output channels we have to\n",
        "        # project the shortcut connection\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        # Linear layer for time embeddings\n",
        "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
        "        self.time_act = Swish()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
        "        * `t` has shape `[batch_size, time_channels]`\n",
        "        \"\"\"\n",
        "        # First convolution layer\n",
        "        h = self.conv1(self.act1(self.norm1(x)))\n",
        "        # Add time embeddings\n",
        "        h += self.time_emb(self.time_act(t))[:, :, None, None]\n",
        "        # Second convolution layer\n",
        "        h = self.conv2(self.dropout(self.act2(self.norm2(h))))\n",
        "\n",
        "        # Add the shortcut connection and return\n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "\n",
        "class AttentionBlock(Module):\n",
        "    \"\"\"\n",
        "    ### Attention block\n",
        "\n",
        "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
        "        \"\"\"\n",
        "        * `n_channels` is the number of channels in the input\n",
        "        * `n_heads` is the number of heads in multi-head attention\n",
        "        * `d_k` is the number of dimensions in each head\n",
        "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Default `d_k`\n",
        "        if d_k is None:\n",
        "            d_k = n_channels\n",
        "        # Normalization layer\n",
        "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
        "        # Projections for query, key and values\n",
        "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
        "        # Linear layer for final transformation\n",
        "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
        "        # Scale for dot-product attention\n",
        "        self.scale = d_k ** -0.5\n",
        "        #\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
        "        * `t` has shape `[batch_size, time_channels]`\n",
        "        \"\"\"\n",
        "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
        "        # to match with `ResidualBlock`.\n",
        "        _ = t\n",
        "        # Get shape\n",
        "        batch_size, n_channels, height, width = x.shape\n",
        "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
        "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
        "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
        "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
        "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
        "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
        "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
        "        attn = attn.softmax(dim=2)\n",
        "        # Multiply by values\n",
        "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
        "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
        "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
        "        # Transform to `[batch_size, seq, n_channels]`\n",
        "        res = self.output(res)\n",
        "\n",
        "        # Add skip connection\n",
        "        res += x\n",
        "\n",
        "        # Change to shape `[batch_size, in_channels, height, width]`\n",
        "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
        "\n",
        "        #\n",
        "        return res\n",
        "\n",
        "\n",
        "class DownBlock(Module):\n",
        "    \"\"\"\n",
        "    ### Down block\n",
        "\n",
        "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res(x, t)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpBlock(Module):\n",
        "    \"\"\"\n",
        "    ### Up block\n",
        "\n",
        "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
        "        # from the first half of the U-Net\n",
        "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res(x, t)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiddleBlock(Module):\n",
        "    \"\"\"\n",
        "    ### Middle block\n",
        "\n",
        "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
        "    This block is applied at the lowest resolution of the U-Net.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int, time_channels: int):\n",
        "        super().__init__()\n",
        "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "        self.attn = AttentionBlock(n_channels)\n",
        "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res1(x, t)\n",
        "        x = self.attn(x)\n",
        "        x = self.res2(x, t)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Scale up the feature map by $2 \\times$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
        "        # to match with `ResidualBlock`.\n",
        "        _ = t\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
        "        # to match with `ResidualBlock`.\n",
        "        _ = t\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(Module):\n",
        "    \"\"\"\n",
        "    ## U-Net\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
        "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
        "                 is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, True, True),\n",
        "                 n_blocks: int = 2):\n",
        "        \"\"\"\n",
        "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
        "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
        "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
        "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
        "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of resolutions\n",
        "        n_resolutions = len(ch_mults)\n",
        "\n",
        "        # Project image into feature map\n",
        "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
        "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
        "\n",
        "        # #### First half of U-Net - decreasing resolution\n",
        "        down = []\n",
        "        # Number of channels\n",
        "        out_channels = in_channels = n_channels\n",
        "        # For each resolution\n",
        "        for i in range(n_resolutions):\n",
        "            # Number of output channels at this resolution\n",
        "            out_channels = in_channels * ch_mults[i]\n",
        "            # Add `n_blocks`\n",
        "            for _ in range(n_blocks):\n",
        "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "                in_channels = out_channels\n",
        "            # Down sample at all resolutions except the last\n",
        "            if i < n_resolutions - 1:\n",
        "                down.append(Downsample(in_channels))\n",
        "\n",
        "        # Combine the set of modules\n",
        "        self.down = nn.ModuleList(down)\n",
        "\n",
        "        # Middle block\n",
        "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
        "\n",
        "        # #### Second half of U-Net - increasing resolution\n",
        "        up = []\n",
        "        # Number of channels\n",
        "        in_channels = out_channels\n",
        "        # For each resolution\n",
        "        for i in reversed(range(n_resolutions)):\n",
        "            # `n_blocks` at the same resolution\n",
        "            out_channels = in_channels\n",
        "            for _ in range(n_blocks):\n",
        "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "            # Final block to reduce the number of channels\n",
        "            out_channels = in_channels // ch_mults[i]\n",
        "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "            in_channels = out_channels\n",
        "            # Up sample at all resolutions except last\n",
        "            if i > 0:\n",
        "                up.append(Upsample(in_channels))\n",
        "\n",
        "        # Combine the set of modules\n",
        "        self.up = nn.ModuleList(up)\n",
        "\n",
        "        # Final normalization and convolution layer\n",
        "        self.norm = nn.GroupNorm(8, n_channels)\n",
        "        self.act = Swish()\n",
        "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
        "        * `t` has shape `[batch_size]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get time-step embeddings\n",
        "        t = self.time_emb(t)\n",
        "\n",
        "        # Get image projection\n",
        "        x = self.image_proj(x)\n",
        "\n",
        "        # `h` will store outputs at each resolution for skip connection\n",
        "        h = [x]\n",
        "        # First half of U-Net\n",
        "        for m in self.down:\n",
        "            x = m(x, t)\n",
        "            h.append(x)\n",
        "\n",
        "        # Middle (bottom)\n",
        "        x = self.middle(x, t)\n",
        "\n",
        "        # Second half of U-Net\n",
        "        for m in self.up:\n",
        "            if isinstance(m, Upsample):\n",
        "                x = m(x, t)\n",
        "            else:\n",
        "                # Get the skip connection from first half of U-Net and concatenate\n",
        "                s = h.pop()\n",
        "                x = torch.cat((x, s), dim=1)\n",
        "                #\n",
        "                x = m(x, t)\n",
        "\n",
        "        # Final normalization and convolution\n",
        "        return self.final(self.act(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5ePQmooOie1U"
      },
      "outputs": [],
      "source": [
        "#@markdown Load the dataset.\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "dataset = load_dataset('mwkldeveloper/sprites_1788_16', split='train')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Resize((32, 32), 0)\n",
        "])\n",
        "\n",
        "\n",
        "def process(examples):\n",
        "    return {\n",
        "        'image': [transform(image) for image in examples['images']]\n",
        "    }\n",
        "\n",
        "dataset.set_transform(process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "397d25226aed4ab796646bcc2d5d1dad",
            "5b0348d04a9a40b696dcecec2d14edc2",
            "e68395692f9e48329275e5a8a73fd370",
            "d2505b74175748929dd69d3a6dff3431",
            "17770442eeb64bff98d406f292787973",
            "8adfa77bb51041c797a831daa66c51d7",
            "707fcd3c00f64032b76ec811cbec2062",
            "bcbdf74a3a464a14ac9e6aff2d13599e",
            "4c3469d51f0545d0873e168ca7471c13",
            "42c99b605f7d4f04a0e5b64639406786",
            "e021bc18adc245d1b49de94a730fe391"
          ]
        },
        "id": "ZZujAln0xPwm",
        "outputId": "2ca64eab-a72d-42d7-963a-4fdb4884986d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "397d25226aed4ab796646bcc2d5d1dad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-079feede51b3>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m trainer.train(\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c7fe372fc8da>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, schedule, optimizer, dataloader, x_column, steps, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatches_per_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;31m# optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREADY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         assert (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@markdown Train the model.\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# schedule = DDPMSchedule(start=1e-4, end=1/20, steps=100)\n",
        "schedule = DDPMSchedule(start=1e-4, end=0.02, steps=1000) # original steps=1000\n",
        "\n",
        "# model = DiT(\n",
        "#     input_size=32,\n",
        "#     patch_size=2,\n",
        "#     in_channels=1,  # 3\n",
        "#     hidden_size=256,\n",
        "#     depth=16,\n",
        "#     num_heads=16,\n",
        "#     mlp_ratio=3,\n",
        "#     num_classes=10,\n",
        "#     learn_sigma=False,\n",
        "# )\n",
        "\n",
        "model = UNet(\n",
        "    image_channels=3,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=16, shuffle=True, num_workers=4)  # original: bs = 16\n",
        "\n",
        "trainer = Trainer(\n",
        "    effective_batch_size=16,#16,\n",
        "    sample_steps=512, #512,\n",
        "    checkpoint_steps=1024,\n",
        ")\n",
        "\n",
        "trainer.train(\n",
        "    model=model,\n",
        "    schedule=schedule,\n",
        "    optimizer=optimizer,\n",
        "    dataloader=dataloader,\n",
        "    x_column='image',\n",
        "    steps=50_000,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Save the model.\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/sprite-diffusion-40k-steps.pt')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uAsrXI-hzcfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "397d25226aed4ab796646bcc2d5d1dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b0348d04a9a40b696dcecec2d14edc2",
              "IPY_MODEL_e68395692f9e48329275e5a8a73fd370",
              "IPY_MODEL_d2505b74175748929dd69d3a6dff3431"
            ],
            "layout": "IPY_MODEL_17770442eeb64bff98d406f292787973"
          }
        },
        "5b0348d04a9a40b696dcecec2d14edc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8adfa77bb51041c797a831daa66c51d7",
            "placeholder": "​",
            "style": "IPY_MODEL_707fcd3c00f64032b76ec811cbec2062",
            "value": "loss: 0.018641:  80%"
          }
        },
        "e68395692f9e48329275e5a8a73fd370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcbdf74a3a464a14ac9e6aff2d13599e",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c3469d51f0545d0873e168ca7471c13",
            "value": 40115
          }
        },
        "d2505b74175748929dd69d3a6dff3431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c99b605f7d4f04a0e5b64639406786",
            "placeholder": "​",
            "style": "IPY_MODEL_e021bc18adc245d1b49de94a730fe391",
            "value": " 40115/50000 [2:35:26&lt;29:51,  5.52it/s]"
          }
        },
        "17770442eeb64bff98d406f292787973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8adfa77bb51041c797a831daa66c51d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707fcd3c00f64032b76ec811cbec2062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcbdf74a3a464a14ac9e6aff2d13599e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3469d51f0545d0873e168ca7471c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42c99b605f7d4f04a0e5b64639406786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e021bc18adc245d1b49de94a730fe391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}